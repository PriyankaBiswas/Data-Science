{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Bot detection \n",
    "\n",
    "Bots, trolls and fake accounts are quite problematic in this era of social media-dependent consciouness. They take part in creating hypes and spams, spread fake news, ultimately displaying non-genuine behavior in micro-blogging platforms. Genuine posts are valuable not only because that help us understand the human sentiments, but also becuase that increase diversity in the sphere.   \n",
    "\n",
    "### Goal \n",
    "The goal of this project is to create a classifier that predicts if an account is a troll or a human. Using APIs, NLP and Machine Learning, I predict important features from which we detect a troll account and avoid spending time communicating with trolls. \n",
    "\n",
    "The outcome from this project could be used for marketers who target real users for selling their products. A curated list of real users will help them reach out to customers easily and attain business goals. \n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "The data comprises existing data from Kaggle [], data collected via Tweepy API from twitter between dates x/x/18 - x/x/18. \n",
    "\n",
    "### Project Workflow\n",
    "\n",
    "#### Approach 1: \n",
    "We tried to build a classifier from existing user account data to predict bots. The model(s) achieved high accuracy, implicating model overfitting. Upon inspection of the datasets, we found we have small training and testing datasets with high sampling bias. Therefore, we needed to gather more data. \n",
    "\n",
    "#### Approach 2:\n",
    "We gathered more data from Twitter. However, we found difficilty in getting  data that could be distinctively labeled for bots and non-bots. Precisely, many Twitter verified accounts that are claimed to be authentic have many bots or they are representative of an organization. The bot data that we gathered [from Website](www.) are small in number (xxx) and are suspended Russian bots. This makes the sample biased.  \n",
    "\n",
    "#### Approach 3:\n",
    "Becuse of this we decided to do a sentiment analysis on #TOPIC and identify bots from the tweets. \n",
    "\n",
    "\n",
    "### Analysis\n",
    "\n",
    "Predicting bots from text is an NLP (Natural Languae Processing) problem.\n",
    "I leveraged NLTK as my tokenizer and Sklearn tfidfVectorizer to perform my Bag of words Analysis and tfidf transformation. Word2Vec\n",
    "\n",
    "For the final model I evaluated model complexity by accuarcy and speed. I eventually settled on _____\n",
    "- Why? and limiting the max features in the SVD step to explain a lot of  __________________\n",
    "\n",
    "-.\n",
    "\n",
    "-.\n",
    "\n",
    "-.\n",
    "\n",
    "\n",
    "### Conclusions\n",
    "- .\n",
    "\n",
    "-.\n",
    "\n",
    "-.\n",
    "\n",
    "-.\n",
    "\n",
    "\n",
    "\n",
    "## Approach 1: Bot detection from user account data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Import sklearn models \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, confusion_matrix, accuracy_score\n",
    "\n",
    "#nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Time \n",
    "import time as t\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to do next - \n",
    "\n",
    " - try creating the histograms for each of the plots above to show how much overlap and the overall distribution of each class. \n",
    "- use the log features for the inputs, probably not required for RFC, \n",
    "- try all the classifiers with the log features and ses if that imporives the accuracy/metrics of the other classifiers. \n",
    "- bring more data - https://botometer.iuni.iu.edu/bot-repository/datasets.html\n",
    "- try more feature engineering - log, and get some more features may be from the paper\n",
    "- try gettin more tweets from the bots and the real users - try sentitment analysis - give each person a score overall on how positive and negative feature. \n",
    "\n",
    "- write up what you did so far - this is the baseline - \n",
    "- 100 % accuracy > more data > how the data is biased > use the cluster plots to show how they are biased - \n",
    "- now bring in some fresh data from the other reposi https://botometer.iuni.iu.edu/bot-repository/datasets.html\n",
    "- feature engineering and see how incorporate NLP and how sentiment analysis to score [positivity/negativity ] for the last 10 tweets. \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of baseline \n",
    "\n",
    "To train our system we initially used a publicly available dataset consisting of 7k bot and non-bot accounts in total. Later we collected data; employed Twitter scraping and incorporated data from various publicly available resources. This procedure yielded a dataset of 57.4k with 39k nonbot and 18k bot accounts. \n",
    "\n",
    "We benchmarked our system using several off-the-shelf algorithms provided in the scikit-learn library (Pedregosa et. al. 2011). We measured the models' accuracy by measuring the Area Under the Receiver Operating Curve (ROC-AUC) with 5-fold cross validation. We compared Bernoulli Naive Bayes, Logistic Regression, Gradient Boosting Classifier, K-nearest neighbors, Random Forest Classifier and Support Vector Classifier. The best classification performance of 98.76 was obtained by the Random Forest Algorithm. The Random Forest model was trained with 100 estimators and the GIni coefficient to measure the quality of splits.\n",
    "\n",
    "For the rest of the project I am going to focus on the latest verified dataset from [this resource](https://botometer.iuni.iu.edu/bot-repository/datasets.html) as the data we collected had sampling bias. Another advantage of choosing the following dataset is that we can access the tweets provided with these datasets. \n",
    "\n",
    "In the following I take a subset of certified bot and non-bot accounts, create features and try to improve the model accuracy. This dataset has almost 39k datapoints with 36% certified bots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting more data \n",
    "all_hums = pd.read_csv('Data/all_hums.csv', encoding='utf-8')\n",
    "all_bots = pd.read_csv('Data/all_bots.csv', encoding='utf-8')\n",
    "new_fake = pd.read_csv('Data/fake_users_.csv')\n",
    "fake_users_2 = pd.read_csv('Data/fake_users_2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating certified humans and non-human dataset \n",
    "cert_hum = all_hums\n",
    "cert_fake = pd.concat([all_bots, fake_users_2, new_fake], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24700, 20), (14245, 20))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cert_hum.shape, cert_fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38945, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cert = pd.concat([cert_hum, cert_fake], axis=0, ignore_index=True)\n",
    "\n",
    "df_cert.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    24700\n",
       "1    14245\n",
       "Name: bot, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cert.bot.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bot                          0\n",
       "created_at                   0\n",
       "default_profile          30220\n",
       "default_profile_image    34654\n",
       "description              24536\n",
       "favourites_count         19276\n",
       "followers_count              0\n",
       "friends_count                0\n",
       "has_extended_profile     38945\n",
       "id                           0\n",
       "id_str                   38945\n",
       "lang                     20276\n",
       "listed_count             19276\n",
       "location                 26097\n",
       "name                     19277\n",
       "screen_name              19276\n",
       "status                   38945\n",
       "statuses_count           19276\n",
       "url                      30544\n",
       "verified                 38934\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cert.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features \n",
    "\n",
    "__Existing features__: So far we have trained our models on the numerical features. And the most imporeant features are 'statuses_count' and 'favourites_count', followed by 'followers_count' and 'friends_count'. In the following I create new features, based on the meta-data provided with the user accounts. However, the main difficulty in creating these features is that a large percentage of the values are missing in these features. \n",
    "\n",
    "__Intended Features from meta-data__:\n",
    "\n",
    "- Length of screen names \n",
    "- Length of description \n",
    "- calculate the ratio of (# of friends/2 * # of followers). It has been claimed that spambots have a high ratio value (i.e., lower ratio values mean legitimate users). [here](https://arxiv.org/pdf/1509.04098.pdf)\n",
    "\n",
    "__Intended Features from tweets__:\n",
    "- The content of spambots’ tweets exhibits the so-called message similarity. The score is higher for bots. \n",
    "- Sentiment features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19276 24536\n"
     ]
    }
   ],
   "source": [
    "# There are several null values or blanks \n",
    "print(df_cert.screen_name.isna().sum(),\n",
    "df_cert.description.isna().sum())\n",
    "\n",
    "#Filling blanks with 0 \n",
    "\n",
    "df_cert.screen_name = df_cert.screen_name.fillna(0)\n",
    "df_cert.description = df_cert.description.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features from meta-data\n",
    "\n",
    "# Length of screen name \n",
    "df_cert.screen_name = df_cert.screen_name.astype('str')\n",
    "df_cert['len_screen_name'] = df_cert.screen_name.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     19276\n",
       "15     3128\n",
       "12     2994\n",
       "13     2590\n",
       "14     2460\n",
       "11     2457\n",
       "10     2023\n",
       "9      1693\n",
       "8      1130\n",
       "7       748\n",
       "6       332\n",
       "5       102\n",
       "4        10\n",
       "3         2\n",
       "Name: len_screen_name, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all null values are converted to 1 . # 19k 1 length \n",
    "\n",
    "df_cert.len_screen_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of description a\n",
    "df_cert.description = df_cert.description.astype('str')  # null values will be converted to 1 in the next step\n",
    "df_cert['len_desc'] = df_cert.description.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cert.len_desc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio of (# of friends/2 * # of followers)  # thankfully no null values \n",
    "\n",
    "df_cert['friend_2xfollower_ratio'] = df_cert\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cert.friends_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
